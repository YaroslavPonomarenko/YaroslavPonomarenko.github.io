<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

    <title>Yaroslav Ponomarenko</title>

    <meta name="author" content="Yaroslav Ponomarenko" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      rel="shortcut icon"
      href="images/favicon/favicon.ico"
      type="image/x-icon" />
    <link rel="stylesheet" type="text/css" href="stylesheet.css" />
  </head>

  <body>
    <table
      style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <h2 class="name" style="text-align: center;">
                      Iaroslav Ponomarenko
                    </h2>
                    <p>
                      <center>
                        yaroslav dot ponomarenko at stu dot pku dot edu dot
                        cn&nbsp;
                      </center>
                    </p>

                    <p>
                      I am a second-year master's student at
                      <a href="https://cs.pku.edu.cn">Peking University</a>'s
                      <a
                        href="https://cfcs.pku.edu.cn/people/graduate_students/index.htm">Center on
                        Frontiers of Computing Studies</a>, where I am fortunate to work with
                      Professor
                      <a href="https://zsdonghao.github.io">Hao Dong</a> at the
                      joint
                      <a
                        href="https://cfcs.pku.edu.cn/english/research/researchlabs/237027.htm">Hyperplane/AGIBot
                        PKU Lab</a>
                      on embodied AI, robotics, large foundation models, and
                      computer vision. Additionally, I am a research intern at
                      <a href="https://agibot.com">AGIBot</a>.
                    </p>
                    <p>
                      Previously, I obtained an Engineering degree in
                      Information Systems and Technologies from
                      <a href="https://vivt.ru">Voronezh Institute of High Technologies</a>, as well
                      as a Technician degree in Automated Information
                      Processing and Control Systems from Borisoglebsk College
                      of Informatics and Computer Engineering.
                    </p>
                    <p style="text-align:center">
                      <!-- <a href="data/YaroslavPonomarenko-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                      <!-- <a href="data/YaroslavPonomarenko-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                      <a
                        href="https://scholar.google.com/citations?hl=en&user=bBFYNasAAAAJ">Google
                        Scholar</a>
                      &nbsp;/&nbsp;
                      <a href="https://www.linkedin.com/in/yaroslavponomarenko/">LinkedIn</a>
                      &nbsp;/&nbsp;
                      <a href="https://github.com/YaroslavPonomarenko">Github</a>
                    </p>
                  </td>
                  <td style="padding:2.5%;width:40%;max-width:40%">
                    <img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;"
                      alt="Iaroslav Ponomarenko"
                      src="data/images/IaroslavPonomarenko.png" />
                  </td>
                </tr>
              </tbody>
            </table>
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <h2>Research</h2>
                    <p>
                      My research focuses on the intersection of embodied AI,
                      perception, reasoning, and robotic control. In particular,
                      I work on affordance understanding [1, 2] and spatial reasoning
                      for embodied agents performing complex manipulation tasks.
                      Currently, I am investigating these areas using large
                      multimodal foundation models.&nbsp; &nbsp; &nbsp;
                    </p>
                    <h2>Selected publications</h2>
                    <!-- <p>(*) denotes equal contribution</p> -->
                  </td>
                </tr>
              </tbody>
            </table>

            <table style="width:100%;border:0;margin:auto;border-spacing:0;">
              <tbody>
                <!-- ManipVQA - Start -->
                <tr
                  onmouseout="manipvqa_stop()"
                  onmouseover="manipvqa_start()"
                  bgcolor="#ffffd0">
                  <td
                    style="padding: 20px 10px 20px 20px; width: 5%; vertical-align: middle;">
                    <p>2</p>
                  </td>

                  <td
                    style="padding: 20px 10px 20px 10px; width: 20%; vertical-align: middle;">
                    <div class="one">
                      <div
                        class="two"
                        id="manipvqa_image"
                        style="position:relative;">
                        <video
                          autoplay
                          loop
                          muted
                          style="width:100%;position:absolute;top:0;left:0;opacity:0;">
                          <source
                            src="data/publications/2024_ManipVQA/ManipVQA_Demo.mp4"
                            type="video/mp4" />
                          Your browser does not support the video tag.
                        </video>
                        <img
                          src="data/publications/2024_ManipVQA/ManipVQA_Demo.jpg"
                          style="width:100%;display:block;" />
                      </div>
                    </div>
                  </td>

                  <td
                    style="padding: 20px 20px 20px 10px; width: 75%; vertical-align: middle;">
                    <a href="https://arxiv.org/pdf/2403.11289v2">
                      <span class="papertitle">ManipVQA: Injecting Robotic Affordance and Physically
                        Grounded Information into Multi-Modal Large Language
                        Models</span>
                    </a>
                    <br />
                    <a
                      href="https://scholar.google.com/citations?user=QNkS4KEAAAAJ&hl=en">Siyuan
                      Huang*</a>, <strong>Yaroslav Ponomarenko*</strong>,
                    <a
                      href="https://scholar.google.com/citations?user=ooBQi6EAAAAJ&hl=en">Zhengkai
                      Jiang</a>,
                    <a
                      href="https://scholar.google.com/citations?user=vkQ5_LIAAAAJ&hl=en">Xiaoqi
                      Li</a>,
                    <a
                      href="https://scholar.google.com/citations?user=3lMuodUAAAAJ&hl=en">Xiaobin
                      Hu</a>,
                    <a
                      href="https://scholar.google.com/citations?user=_go6DPsAAAAJ&hl=en">Peng
                      Gao</a>,
                    <a
                      href="https://scholar.google.com/citations?user=BN2Ze-QAAAAJ&hl=en">Hongsheng
                      Li</a>,
                    <a
                      href="https://scholar.google.com/citations?user=xLFL4sMAAAAJ&hl=en">Hao
                      Dong†</a>
                    <br />
                    <em>IROS</em>, 2024
                    &nbsp;&nbsp;&nbsp;&nbsp;<strong><span style="color: red;">[Oral Pitch and
                        Interactive
                        Presentation]</span></strong>
                    <br />
                    <a href="https://arxiv.org/abs/2403.11289v2">arXiv</a>
                    / <a href="https://github.com/SiyuanHuang95/ManipVQA">GitHub</a>
                    <p>
                      We introduce ManipVQA, a unified VQA format training
                      dataset, model, and a robust framework for integrating
                      physical knowledge and affordance reasoning into
                      multimodal large language models (MLLMs).
                    </p>
                  </td>

                  <script type="text/javascript">
                    function manipvqa_start() {
                      document.querySelector('#manipvqa_image video').style.opacity = "1";
                    }
                    function manipvqa_stop() {
                      document.querySelector('#manipvqa_image video').style.opacity = "0";
                    }
                    manipvqa_stop();
                  </script>
                </tr>
                <!-- ManipVQA - End -->

                <!-- LPAAV3DOM - Start -->
                <tr
                  onmouseout="LPAAV3DOM_stop()"
                  onmouseover="LPAAV3DOM_start()"
                  bgcolor="#FFFFFF">
                  <td
                    style="padding: 20px 10px 20px 20px; width: 5%; vertical-align: middle;">
                    <p>1</p>
                  </td>

                  <td
                    style="padding: 20px 10px 20px 10px; width: 20%; vertical-align: middle;">
                    <div class="one">
                      <div
                        class="two"
                        id="LPAAV3DOM_image"
                        style="position:relative;">
                        <video
                          autoplay
                          loop
                          muted
                          style="width:100%;position:absolute;top:0;left:0;opacity:0;">
                          <source
                            src
                            type="video/mp4" />
                          Your browser does not support the video tag.
                        </video>
                        <img
                          src="data/publications/2023_LPAAV3DOM/LPAAV3DOM_Demo.jpg"
                          style="width:100%;display:block;" />
                      </div>
                    </div>
                  </td>

                  <td
                    style="padding: 20px 20px 20px 10px; width: 75%; vertical-align: middle;">
                    <a
                      href="https://drive.google.com/file/d/1ESca7sAJ4CsH3R6H33shp0iodlp1OlKw/view?usp=drive_link">
                      <span class="papertitle">Learning Part-Aware Visual Actionable Affordance for
                        3D Articulated Object Manipulation</span>
                    </a>
                    <br />
                    <a href="https://scholar.google.com/citations?hl=en&user=jOPXmhIAAAAJ">Yuanchen
                      Ju*</a>,
                    <a href="https://scholar.google.com/citations?user=Inr-6rEAAAAJ">Haoran
                      Geng*</a>,
                    Ming Yang*,
                    <a href="https://scholar.google.com/citations?user=q22ys2QAAAAJ&hl">Yiran
                      Geng</a>,
                    <strong>Yaroslav Ponomarenko</strong>,
                    <a href="https://www.linkedin.com/in/taewhan-kim-3122621a7/">Taewhan Kim</a>,
                    <a
                      href="https://scholar.google.com/citations?hl=en&user=roCAWkoAAAAJ&view_op=list_works&sortby=pubdate">He
                      Wang</a>,
                    <a
                      href="https://scholar.google.com/citations?user=xLFL4sMAAAAJ&hl=en">Hao
                      Dong†</a>
                    <br />
                    <em>CVPR @ 3DVR</em>, 2023
                    &nbsp;&nbsp;&nbsp;&nbsp;<strong><span style="color: red;">[Spotlight
                        Presentation]</span></strong>
                    <br />
                    <a
                      href="https://drive.google.com/file/d/1ESca7sAJ4CsH3R6H33shp0iodlp1OlKw/view?usp=drive_link">Paper</a>
                    /
                    <a href="https://sites.google.com/view/cvpr2023-3d-vision-robotics">
                      Workshop</a>
                    <p>
                      We introduce Part-aware Affordance Learning methods. Our approach initially
                      learns a prior for object parts and then generates an affordance map. To
                      further improve precision, we incorporate a part-level scoring system to
                      identify the most suitable part for manipulation.
                    </p>
                  </td>

                  <script type="text/javascript">
                    function LPAAV3DOM_start() {
                      document.querySelector('#LPAAV3DOM_image video').style.opacity = "1";
                    }
                    function LPAAV3DOM_stop() {
                      document.querySelector('#LPAAV3DOM_image video').style.opacity = "0";
                    }
                    LPAAV3DOM_stop();
                  </script>
                </tr>
                <!-- LPAAV3DOM - End -->

              </tbody>
            </table>
          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
